{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Job Names\n",
    "\n",
    "AWS: 'image-classification-200112-2133-002-5c8e0b5a'<br>\n",
    "MyCNN: 'pytorch-training-2020-01-19-08-01-36-460'<br>\n",
    "VGG-16: 'pytorch-training-2020-01-20-23-41-49-889'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-12 23:14:27 Starting - Preparing the instances for training\n",
      "2020-01-12 23:14:27 Downloading - Downloading input data\n",
      "2020-01-12 23:14:27 Training - Training image download completed. Training in progress.\n",
      "2020-01-12 23:14:27 Uploading - Uploading generated training model\n",
      "2020-01-12 23:14:27 Completed - Training job completed\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.05561869868809854', u'optimizer': u'adam', u'_tuning_objective_metric': u'validation:accuracy', u'precision_dtype': u'float32', u'epochs': u'10', u'num_training_samples': u'5215', u'num_layers': u'152', u'mini_batch_size': u'32', u'image_shape': u'3,224,224', u'num_classes': u'2'}\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Final configuration: {u'optimizer': u'adam', u'_tuning_objective_metric': u'validation:accuracy', u'learning_rate': u'0.05561869868809854', u'epochs': u'10', u'lr_scheduler_factor': 0.1, u'num_layers': u'152', u'precision_dtype': u'float32', u'mini_batch_size': u'32', u'num_classes': u'2', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': 0, u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'5215'}\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Creating record files for training.lst\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Done creating record files...\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Creating record files for validation.lst\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Done creating record files...\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] multi_label: 0\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Performing random weight initialization\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] num_layers: 152\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] epochs: 10\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] optimizer: adam\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] beta_1: 0.9\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] beta_2: 0.999\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] eps: 1e-08\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] learning_rate: 0.0556186986881\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] num_training_samples: 5215\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] mini_batch_size: 32\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] num_classes: 2\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] kv_store: device\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] tuning_objective_metric: validation:accuracy\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] --------------------\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[22:28:38] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1888.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:29:19 INFO 139726532032320] Epoch[0] Batch [20]#011Speed: 15.148 samples/sec#011accuracy=0.700893\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:29:51 INFO 139726532032320] Epoch[0] Batch [40]#011Speed: 17.215 samples/sec#011accuracy=0.727896\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:30:24 INFO 139726532032320] Epoch[0] Batch [60]#011Speed: 18.003 samples/sec#011accuracy=0.736168\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:30:56 INFO 139726532032320] Epoch[0] Batch [80]#011Speed: 18.406 samples/sec#011accuracy=0.737654\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:31:29 INFO 139726532032320] Epoch[0] Batch [100]#011Speed: 18.643 samples/sec#011accuracy=0.738861\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:32:01 INFO 139726532032320] Epoch[0] Batch [120]#011Speed: 18.799 samples/sec#011accuracy=0.735795\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:32:34 INFO 139726532032320] Epoch[0] Batch [140]#011Speed: 18.903 samples/sec#011accuracy=0.738918\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:07 INFO 139726532032320] Epoch[0] Batch [160]#011Speed: 18.980 samples/sec#011accuracy=0.740295\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:08 INFO 139726532032320] Epoch[0] Train-accuracy=0.740934\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:08 INFO 139726532032320] Epoch[0] Time cost=271.388\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:08 INFO 139726532032320] Epoch[0] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:43 INFO 139726532032320] Epoch[1] Batch [20]#011Speed: 18.751 samples/sec#011accuracy=0.763393\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:34:16 INFO 139726532032320] Epoch[1] Batch [40]#011Speed: 19.130 samples/sec#011accuracy=0.798780\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:34:48 INFO 139726532032320] Epoch[1] Batch [60]#011Speed: 19.271 samples/sec#011accuracy=0.815061\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:35:21 INFO 139726532032320] Epoch[1] Batch [80]#011Speed: 19.334 samples/sec#011accuracy=0.823302\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:35:54 INFO 139726532032320] Epoch[1] Batch [100]#011Speed: 19.374 samples/sec#011accuracy=0.827661\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:36:27 INFO 139726532032320] Epoch[1] Batch [120]#011Speed: 19.414 samples/sec#011accuracy=0.836519\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:36:59 INFO 139726532032320] Epoch[1] Batch [140]#011Speed: 19.437 samples/sec#011accuracy=0.842199\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:32 INFO 139726532032320] Epoch[1] Batch [160]#011Speed: 19.450 samples/sec#011accuracy=0.850155\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:34 INFO 139726532032320] Epoch[1] Train-accuracy=0.850887\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:34 INFO 139726532032320] Epoch[1] Time cost=264.871\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:35 INFO 139726532032320] Epoch[1] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:36 INFO 139726532032320] Storing the best model with validation accuracy: 0.500000\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:36 INFO 139726532032320] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:38:10 INFO 139726532032320] Epoch[2] Batch [20]#011Speed: 18.803 samples/sec#011accuracy=0.900298\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:38:43 INFO 139726532032320] Epoch[2] Batch [40]#011Speed: 19.155 samples/sec#011accuracy=0.912348\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:39:16 INFO 139726532032320] Epoch[2] Batch [60]#011Speed: 19.284 samples/sec#011accuracy=0.915984\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:39:48 INFO 139726532032320] Epoch[2] Batch [80]#011Speed: 19.352 samples/sec#011accuracy=0.914352\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:40:21 INFO 139726532032320] Epoch[2] Batch [100]#011Speed: 19.395 samples/sec#011accuracy=0.918007\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:40:54 INFO 139726532032320] Epoch[2] Batch [120]#011Speed: 19.423 samples/sec#011accuracy=0.919680\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:41:26 INFO 139726532032320] Epoch[2] Batch [140]#011Speed: 19.442 samples/sec#011accuracy=0.920213\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:41:59 INFO 139726532032320] Epoch[2] Batch [160]#011Speed: 19.455 samples/sec#011accuracy=0.922943\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:01 INFO 139726532032320] Epoch[2] Train-accuracy=0.923225\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:01 INFO 139726532032320] Epoch[2] Time cost=264.813\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:01 INFO 139726532032320] Epoch[2] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:36 INFO 139726532032320] Epoch[3] Batch [20]#011Speed: 18.662 samples/sec#011accuracy=0.936012\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:43:08 INFO 139726532032320] Epoch[3] Batch [40]#011Speed: 19.102 samples/sec#011accuracy=0.952744\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:43:41 INFO 139726532032320] Epoch[3] Batch [60]#011Speed: 19.237 samples/sec#011accuracy=0.946721\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:44:14 INFO 139726532032320] Epoch[3] Batch [80]#011Speed: 19.315 samples/sec#011accuracy=0.944444\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:44:47 INFO 139726532032320] Epoch[3] Batch [100]#011Speed: 19.362 samples/sec#011accuracy=0.946163\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:45:19 INFO 139726532032320] Epoch[3] Batch [120]#011Speed: 19.396 samples/sec#011accuracy=0.945506\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:45:52 INFO 139726532032320] Epoch[3] Batch [140]#011Speed: 19.420 samples/sec#011accuracy=0.945922\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:25 INFO 139726532032320] Epoch[3] Batch [160]#011Speed: 19.440 samples/sec#011accuracy=0.946040\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:26 INFO 139726532032320] Epoch[3] Train-accuracy=0.945988\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:26 INFO 139726532032320] Epoch[3] Time cost=265.016\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:28 INFO 139726532032320] Epoch[3] Validation-accuracy=0.687500\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:28 INFO 139726532032320] Storing the best model with validation accuracy: 0.687500\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:29 INFO 139726532032320] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:47:03 INFO 139726532032320] Epoch[4] Batch [20]#011Speed: 18.771 samples/sec#011accuracy=0.958333\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:47:36 INFO 139726532032320] Epoch[4] Batch [40]#011Speed: 19.140 samples/sec#011accuracy=0.946646\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:48:08 INFO 139726532032320] Epoch[4] Batch [60]#011Speed: 19.274 samples/sec#011accuracy=0.945184\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:48:41 INFO 139726532032320] Epoch[4] Batch [80]#011Speed: 19.343 samples/sec#011accuracy=0.943673\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:49:14 INFO 139726532032320] Epoch[4] Batch [100]#011Speed: 19.379 samples/sec#011accuracy=0.943069\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:49:47 INFO 139726532032320] Epoch[4] Batch [120]#011Speed: 19.407 samples/sec#011accuracy=0.943440\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:19 INFO 139726532032320] Epoch[4] Batch [140]#011Speed: 19.431 samples/sec#011accuracy=0.943484\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:52 INFO 139726532032320] Epoch[4] Batch [160]#011Speed: 19.450 samples/sec#011accuracy=0.942741\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:54 INFO 139726532032320] Epoch[4] Train-accuracy=0.942515\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:54 INFO 139726532032320] Epoch[4] Time cost=264.883\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:54 INFO 139726532032320] Epoch[4] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:51:29 INFO 139726532032320] Epoch[5] Batch [20]#011Speed: 18.713 samples/sec#011accuracy=0.949405\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:52:01 INFO 139726532032320] Epoch[5] Batch [40]#011Speed: 19.127 samples/sec#011accuracy=0.961128\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:52:34 INFO 139726532032320] Epoch[5] Batch [60]#011Speed: 19.267 samples/sec#011accuracy=0.957992\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:53:07 INFO 139726532032320] Epoch[5] Batch [80]#011Speed: 19.343 samples/sec#011accuracy=0.957948\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:53:39 INFO 139726532032320] Epoch[5] Batch [100]#011Speed: 19.387 samples/sec#011accuracy=0.948948\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:54:12 INFO 139726532032320] Epoch[5] Batch [120]#011Speed: 19.413 samples/sec#011accuracy=0.945764\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:54:45 INFO 139726532032320] Epoch[5] Batch [140]#011Speed: 19.432 samples/sec#011accuracy=0.946365\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:18 INFO 139726532032320] Epoch[5] Batch [160]#011Speed: 19.450 samples/sec#011accuracy=0.947399\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:19 INFO 139726532032320] Epoch[5] Train-accuracy=0.947724\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:19 INFO 139726532032320] Epoch[5] Time cost=264.877\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:21 INFO 139726532032320] Epoch[5] Validation-accuracy=0.625000\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:55 INFO 139726532032320] Epoch[6] Batch [20]#011Speed: 18.794 samples/sec#011accuracy=0.943452\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:56:28 INFO 139726532032320] Epoch[6] Batch [40]#011Speed: 19.166 samples/sec#011accuracy=0.958079\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:57:01 INFO 139726532032320] Epoch[6] Batch [60]#011Speed: 19.281 samples/sec#011accuracy=0.952869\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:57:34 INFO 139726532032320] Epoch[6] Batch [80]#011Speed: 19.349 samples/sec#011accuracy=0.954475\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:58:06 INFO 139726532032320] Epoch[6] Batch [100]#011Speed: 19.384 samples/sec#011accuracy=0.955446\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:58:39 INFO 139726532032320] Epoch[6] Batch [120]#011Speed: 19.418 samples/sec#011accuracy=0.957128\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:12 INFO 139726532032320] Epoch[6] Batch [140]#011Speed: 19.437 samples/sec#011accuracy=0.958555\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:45 INFO 139726532032320] Epoch[6] Batch [160]#011Speed: 19.455 samples/sec#011accuracy=0.957880\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:46 INFO 139726532032320] Epoch[6] Train-accuracy=0.957948\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:46 INFO 139726532032320] Epoch[6] Time cost=264.808\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:46 INFO 139726532032320] Epoch[6] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:00:21 INFO 139726532032320] Epoch[7] Batch [20]#011Speed: 18.743 samples/sec#011accuracy=0.955357\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:00:54 INFO 139726532032320] Epoch[7] Batch [40]#011Speed: 19.145 samples/sec#011accuracy=0.963415\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:01:26 INFO 139726532032320] Epoch[7] Batch [60]#011Speed: 19.287 samples/sec#011accuracy=0.962602\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:01:59 INFO 139726532032320] Epoch[7] Batch [80]#011Speed: 19.356 samples/sec#011accuracy=0.961420\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:02:32 INFO 139726532032320] Epoch[7] Batch [100]#011Speed: 19.404 samples/sec#011accuracy=0.957611\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:03:04 INFO 139726532032320] Epoch[7] Batch [120]#011Speed: 19.436 samples/sec#011accuracy=0.958161\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:03:37 INFO 139726532032320] Epoch[7] Batch [140]#011Speed: 19.459 samples/sec#011accuracy=0.958777\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:10 INFO 139726532032320] Epoch[7] Batch [160]#011Speed: 19.472 samples/sec#011accuracy=0.958657\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:11 INFO 139726532032320] Epoch[7] Train-accuracy=0.958333\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:11 INFO 139726532032320] Epoch[7] Time cost=264.585\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:13 INFO 139726532032320] Epoch[7] Validation-accuracy=0.625000\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:47 INFO 139726532032320] Epoch[8] Batch [20]#011Speed: 18.793 samples/sec#011accuracy=0.950893\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:05:20 INFO 139726532032320] Epoch[8] Batch [40]#011Speed: 19.178 samples/sec#011accuracy=0.959604\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:05:53 INFO 139726532032320] Epoch[8] Batch [60]#011Speed: 19.305 samples/sec#011accuracy=0.961578\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:06:26 INFO 139726532032320] Epoch[8] Batch [80]#011Speed: 19.362 samples/sec#011accuracy=0.962577\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:06:58 INFO 139726532032320] Epoch[8] Batch [100]#011Speed: 19.405 samples/sec#011accuracy=0.958230\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:07:31 INFO 139726532032320] Epoch[8] Batch [120]#011Speed: 19.439 samples/sec#011accuracy=0.955579\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:04 INFO 139726532032320] Epoch[8] Batch [140]#011Speed: 19.466 samples/sec#011accuracy=0.957668\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:36 INFO 139726532032320] Epoch[8] Batch [160]#011Speed: 19.482 samples/sec#011accuracy=0.956910\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:38 INFO 139726532032320] Epoch[8] Train-accuracy=0.956983\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:38 INFO 139726532032320] Epoch[8] Time cost=264.442\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:38 INFO 139726532032320] Epoch[8] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:09:12 INFO 139726532032320] Epoch[9] Batch [20]#011Speed: 18.743 samples/sec#011accuracy=0.964286\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:09:45 INFO 139726532032320] Epoch[9] Batch [40]#011Speed: 19.158 samples/sec#011accuracy=0.964177\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:10:18 INFO 139726532032320] Epoch[9] Batch [60]#011Speed: 19.306 samples/sec#011accuracy=0.963627\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:10:50 INFO 139726532032320] Epoch[9] Batch [80]#011Speed: 19.377 samples/sec#011accuracy=0.967978\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:11:23 INFO 139726532032320] Epoch[9] Batch [100]#011Speed: 19.421 samples/sec#011accuracy=0.967203\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:11:56 INFO 139726532032320] Epoch[9] Batch [120]#011Speed: 19.451 samples/sec#011accuracy=0.966942\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:12:28 INFO 139726532032320] Epoch[9] Batch [140]#011Speed: 19.472 samples/sec#011accuracy=0.966312\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:01 INFO 139726532032320] Epoch[9] Batch [160]#011Speed: 19.491 samples/sec#011accuracy=0.965839\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:03 INFO 139726532032320] Epoch[9] Train-accuracy=0.966049\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:03 INFO 139726532032320] Epoch[9] Time cost=264.317\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:04 INFO 139726532032320] Epoch[9] Validation-accuracy=0.937500\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:05 INFO 139726532032320] Storing the best model with validation accuracy: 0.937500\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:05 INFO 139726532032320] Saved checkpoint to \"/opt/ml/model/image-classification-0010.params\"\u001b[0m\n",
      "Training seconds: 2860\n",
      "Billable seconds: 2860\n"
     ]
    }
   ],
   "source": [
    "aws_estimator = sagemaker.estimator.Estimator.attach('image-classification-200112-2133-002-5c8e0b5a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the previous Jupyter Notebook, these are the calculated metrics:\n",
    "\n",
    "## AWS Model <br>\n",
    "\n",
    "True Positives: 379<br>\n",
    "False Positives: 81<br>\n",
    "True Negatives: 153<br>\n",
    "False Negatives: 11<br>\n",
    "\n",
    "AUC Score: 0.8128205128205128 (Pretty good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 08:21:37 Starting - Preparing the instances for training\n",
      "2020-01-19 08:21:37 Downloading - Downloading input data\n",
      "2020-01-19 08:21:37 Training - Training image download completed. Training in progress.\n",
      "2020-01-19 08:21:37 Uploading - Uploading generated training model\n",
      "2020-01-19 08:21:37 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:33,657 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:33,682 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:36,800 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:37,091 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:37,091 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:37,091 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:37,091 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpzqm7kw_p/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=11731 sha256=51f90c02f2fe129b44dcd0bb7f697d3cf76fa9aa05422c87575903852416f4d0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hfl2qffy/wheels/7d/fe/40/0e4f762eb547a4fddbddeac96465b547e091857b8d0a686c8e\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-01-19 08:06:39,488 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 20,\n",
      "        \"learning_rate\": 0.045\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-01-19-08-01-36-460\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-991170486756/pytorch-training-2020-01-19-08-01-36-460/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TrainandTest\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TrainandTest.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":20,\"learning_rate\":0.045}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TrainandTest.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TrainandTest\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-991170486756/pytorch-training-2020-01-19-08-01-36-460/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":20,\"learning_rate\":0.045},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-01-19-08-01-36-460\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-991170486756/pytorch-training-2020-01-19-08-01-36-460/source/sourcedir.tar.gz\",\"module_name\":\"TrainandTest\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TrainandTest.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"20\",\"--learning_rate\",\"0.045\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.045\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python TrainandTest.py --epochs 20 --learning_rate 0.045\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\u001b[0m\n",
      "\u001b[34mStarting to train...\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\u001b[0m\n",
      "\u001b[34m[2020-01-19 08:06:48.300 algo-1:44 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\u001b[0m\n",
      "\u001b[34m[2020-01-19 08:06:48.313 algo-1:44 INFO hook.py:151] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\u001b[0m\n",
      "\u001b[34m[2020-01-19 08:06:48.313 algo-1:44 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-01-19 08:06:48.313 algo-1:44 INFO hook.py:325] Monitoring the collections: losses, scalars\u001b[0m\n",
      "\u001b[34mEpoch: 1 #011Training Loss: 0.637316 #011Validation Loss: 0.739167\u001b[0m\n",
      "\u001b[34mValidation loss decreased (inf --> 0.739167).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 2 #011Training Loss: 0.577723 #011Validation Loss: 0.793170\u001b[0m\n",
      "\u001b[34mEpoch: 3 #011Training Loss: 0.572032 #011Validation Loss: 0.810761\u001b[0m\n",
      "\u001b[34mEpoch: 4 #011Training Loss: 0.569790 #011Validation Loss: 0.823527\u001b[0m\n",
      "\u001b[34mEpoch: 5 #011Training Loss: 0.569966 #011Validation Loss: 0.823605\u001b[0m\n",
      "\u001b[34mEpoch: 6 #011Training Loss: 0.569683 #011Validation Loss: 0.826541\u001b[0m\n",
      "\u001b[34mEpoch: 7 #011Training Loss: 0.568673 #011Validation Loss: 0.825850\u001b[0m\n",
      "\u001b[34mEpoch: 8 #011Training Loss: 0.569239 #011Validation Loss: 0.825718\u001b[0m\n",
      "\u001b[34mEpoch: 9 #011Training Loss: 0.568359 #011Validation Loss: 0.826311\u001b[0m\n",
      "\u001b[34mEpoch: 10 #011Training Loss: 0.567640 #011Validation Loss: 0.820888\u001b[0m\n",
      "\u001b[34mEpoch: 11 #011Training Loss: 0.565406 #011Validation Loss: 0.819053\u001b[0m\n",
      "\u001b[34mEpoch: 12 #011Training Loss: 0.553878 #011Validation Loss: 0.797824\u001b[0m\n",
      "\u001b[34mEpoch: 13 #011Training Loss: 0.467700 #011Validation Loss: 0.657135\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.739167 --> 0.657135).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 14 #011Training Loss: 0.265465 #011Validation Loss: 0.649438\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.657135 --> 0.649438).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 15 #011Training Loss: 0.156855 #011Validation Loss: 0.837287\u001b[0m\n",
      "\u001b[34mEpoch: 16 #011Training Loss: 0.128676 #011Validation Loss: 0.946464\u001b[0m\n",
      "\u001b[34mEpoch: 17 #011Training Loss: 0.125113 #011Validation Loss: 0.574887\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.649438 --> 0.574887).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 18 #011Training Loss: 0.110058 #011Validation Loss: 0.545919\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.574887 --> 0.545919).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 19 #011Training Loss: 0.098661 #011Validation Loss: 0.879289\u001b[0m\n",
      "\u001b[34mEpoch: 20 #011Training Loss: 0.095752 #011Validation Loss: 0.288467\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.545919 --> 0.288467).  Saving model ...\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNow After Training, we can now test the images to calculate the Accuracy and AUC Score\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTotal Test Loss: 0.3678636565804482\n",
      "\u001b[0m\n",
      "\u001b[34mTrue Positives: 370\u001b[0m\n",
      "\u001b[34mFalse Positives: 73\u001b[0m\n",
      "\u001b[34mTrue Negatives: 161\u001b[0m\n",
      "\u001b[34mFalse Negatives: 20\n",
      "\u001b[0m\n",
      "\u001b[34mAccuracy: 0.8509615384615384\u001b[0m\n",
      "\u001b[34mRecall: 0.9487179487179487\u001b[0m\n",
      "\u001b[34mPrecision: 0.835214446952596\u001b[0m\n",
      "\u001b[34mAUC SCORE: 0.8183760683760684\u001b[0m\n",
      "\u001b[34m[2020-01-19 08:18:11.897 algo-1:44 INFO utils.py:27] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-01-19 08:18:12,232 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1058\n",
      "Billable seconds: 1058\n"
     ]
    }
   ],
   "source": [
    "myCNN = sagemaker.estimator.Estimator.attach('pytorch-training-2020-01-19-08-01-36-460')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My CNN\n",
    "Epochs: 20<br>\n",
    "Learning Rate: 0.045<br>\n",
    "\n",
    "True Positives: 370<br>\n",
    "False Positives: 73<br>\n",
    "True Negatives: 161<br>\n",
    "False Negatives: 20<br>\n",
    "\n",
    "AUC Score: 0.8183760683760684 (Better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-21 00:34:48 Starting - Preparing the instances for training\n",
      "2020-01-21 00:34:48 Downloading - Downloading input data\n",
      "2020-01-21 00:34:48 Training - Training image download completed. Training in progress.\n",
      "2020-01-21 00:34:48 Uploading - Uploading generated training model\n",
      "2020-01-21 00:34:48 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,239 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,264 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,265 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,530 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,531 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,531 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:46,531 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpvy7hyvsp/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=15710 sha256=f59282531bfc8acfce3f7b70bdbbad5fc28b262ade37b1cf9e66769b4b4995ca\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vp8jrt1p/wheels/f8/3f/61/80115ba8bf7e3e22ddc3a4b2f56f7d4987c8647f4dd9d2201f\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-01-20 23:46:48,769 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 20,\n",
      "        \"learning_rate\": 0.05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-01-20-23-41-49-889\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-991170486756/pytorch-training-2020-01-20-23-41-49-889/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TrainandTest\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TrainandTest.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":20,\"learning_rate\":0.05}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TrainandTest.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TrainandTest\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-991170486756/pytorch-training-2020-01-20-23-41-49-889/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":20,\"learning_rate\":0.05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-01-20-23-41-49-889\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-991170486756/pytorch-training-2020-01-20-23-41-49-889/source/sourcedir.tar.gz\",\"module_name\":\"TrainandTest\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TrainandTest.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"20\",\"--learning_rate\",\"0.05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.05\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python TrainandTest.py --epochs 20 --learning_rate 0.05\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\u001b[0m\n",
      "\u001b[34mDownloading the ResNeXt-101 32x8d model\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\u001b[0m\n",
      "\u001b[34mReplacing the model's Fully Connected Layer\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\u001b[0m\n",
      "\u001b[34mStarting to train...\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\u001b[0m\n",
      "\u001b[34m[2020-01-20 23:46:56.217 algo-1:44 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\u001b[0m\n",
      "\u001b[34m[2020-01-20 23:46:56.231 algo-1:44 INFO hook.py:151] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\u001b[0m\n",
      "\u001b[34m[2020-01-20 23:46:56.231 algo-1:44 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-01-20 23:46:56.232 algo-1:44 INFO hook.py:325] Monitoring the collections: scalars, losses\u001b[0m\n",
      "\u001b[34mEpoch: 1 #011Training Loss: 0.596083 #011Validation Loss: 0.825511\u001b[0m\n",
      "\u001b[34mValidation loss decreased (inf --> 0.825511).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 2 #011Training Loss: 0.562854 #011Validation Loss: 0.745037\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.825511 --> 0.745037).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 3 #011Training Loss: 0.539856 #011Validation Loss: 0.738819\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.745037 --> 0.738819).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 4 #011Training Loss: 0.320827 #011Validation Loss: 0.415765\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.738819 --> 0.415765).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 5 #011Training Loss: 0.186295 #011Validation Loss: 0.799743\u001b[0m\n",
      "\u001b[34mEpoch: 6 #011Training Loss: 0.150813 #011Validation Loss: 1.007093\u001b[0m\n",
      "\u001b[34mEpoch: 7 #011Training Loss: 0.123687 #011Validation Loss: 1.247123\u001b[0m\n",
      "\u001b[34mEpoch: 8 #011Training Loss: 0.116713 #011Validation Loss: 0.491831\u001b[0m\n",
      "\u001b[34mEpoch: 9 #011Training Loss: 0.099970 #011Validation Loss: 0.277745\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.415765 --> 0.277745).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 10 #011Training Loss: 0.083498 #011Validation Loss: 1.133675\u001b[0m\n",
      "\u001b[34mEpoch: 11 #011Training Loss: 0.094665 #011Validation Loss: 0.327776\u001b[0m\n",
      "\u001b[34mEpoch: 12 #011Training Loss: 0.070494 #011Validation Loss: 0.396194\u001b[0m\n",
      "\u001b[34mEpoch: 13 #011Training Loss: 0.070141 #011Validation Loss: 0.716107\u001b[0m\n",
      "\u001b[34mEpoch: 14 #011Training Loss: 0.057996 #011Validation Loss: 0.618016\u001b[0m\n",
      "\u001b[34mEpoch: 15 #011Training Loss: 0.047080 #011Validation Loss: 0.362875\u001b[0m\n",
      "\u001b[34mEpoch: 16 #011Training Loss: 0.077400 #011Validation Loss: 0.186884\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.277745 --> 0.186884).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 17 #011Training Loss: 0.042027 #011Validation Loss: 0.246728\u001b[0m\n",
      "\u001b[34mEpoch: 18 #011Training Loss: 0.037458 #011Validation Loss: 0.083680\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.186884 --> 0.083680).  Saving model ...\u001b[0m\n",
      "\u001b[34mEpoch: 19 #011Training Loss: 0.285692 #011Validation Loss: 0.652996\u001b[0m\n",
      "\u001b[34mEpoch: 20 #011Training Loss: 0.068828 #011Validation Loss: 0.238943\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNow After Training, we can now test the images to calculate the Accuracy and AUC Score\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTotal Test Loss: 1.0093304812631687\n",
      "\u001b[0m\n",
      "\u001b[34mTrue Positives: 389\u001b[0m\n",
      "\u001b[34mFalse Positives: 128\u001b[0m\n",
      "\u001b[34mTrue Negatives: 106\u001b[0m\n",
      "\u001b[34mFalse Negatives: 1\n",
      "\u001b[0m\n",
      "\u001b[34mAccuracy: 0.7932692307692307\u001b[0m\n",
      "\u001b[34mRecall: 0.9974358974358974\u001b[0m\n",
      "\u001b[34mPrecision: 0.7524177949709865\u001b[0m\n",
      "\u001b[34mAUC SCORE: 0.7252136752136752\u001b[0m\n",
      "\u001b[34m[2020-01-21 00:34:19.849 algo-1:44 INFO utils.py:27] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-01-21 00:34:20,214 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 3001\n",
      "Billable seconds: 3001\n"
     ]
    }
   ],
   "source": [
    "vgg_16 = sagemaker.estimator.Estimator.attach('pytorch-training-2020-01-20-23-41-49-889')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Model <br>\n",
    "Epochs: 20<br>\n",
    "Learning Rate: 0.05<br>\n",
    "\n",
    "True Positives: 389<br>\n",
    "False Positives: 128<br>\n",
    "True Negatives: 106<br>\n",
    "False Negatives: 1<br>\n",
    "\n",
    "AUC Score: 0.7252136752136752 (Better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdict of VGG-16\n",
    "\n",
    "The AUC Score of the best VGG-16 model has a difference of about 0.10 compared to the other two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005555555555555536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the difference of the AUC Scores of the AWS and CNN models\n",
    "aws_auc = 0.8128205128205128\n",
    "cnn_auc = 0.8183760683760684\n",
    "\n",
    "cnn_auc - aws_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS vs My CNN\n",
    "\n",
    "The difference of the AUC scores from above is quite small, so we should look at the other measures.\n",
    "\n",
    "## AWS Model\n",
    "\n",
    "True Positives: 379<br>\n",
    "False Positives: 81<br>\n",
    "True Negatives: 153<br>\n",
    "False Negatives: 11<br>\n",
    "\n",
    "## My CNN\n",
    "\n",
    "True Positives: 370<br>\n",
    "False Positives: 73<br>\n",
    "True Negatives: 161<br>\n",
    "False Negatives: 20<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros for AWS\n",
    "\n",
    "The AWS model was able to correctly predict more images with pneumonia. AWS was also able to have a lower count of False Negatives.\n",
    "\n",
    "## Cons for AWS\n",
    "\n",
    "The AWS had more counts of False Negatives and was not able to correctly predict normal images as well as the CNN Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros for My CNN\n",
    "\n",
    "The CNN was able to have a lower count of False Positives and was able to predict normal images better than the AWS Model.\n",
    "\n",
    "## Cons for My CNN\n",
    "\n",
    "The CNN was not able to predict images with pneumonia as well as the AWS Model. But most importantly, the False Negatives count was higher for the CNN than the AWS model. False Negative count is important because the classifier predicted that the patient was not diagnosed with pneumonia, however the patient is actually ill with pneumonia<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model\n",
    "\n",
    "The best model is the AWS model. My reasoning is that the risk of having a higher True Negative count is not worth having a higher False Negative count as stated above in the CNN.\n",
    "\n",
    "Therefore, its best to use the AWS model since it is less likely to have patients with pneumonia to be diagnosed of not having pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all resources of local machine since it is no longer needed\n",
    "#WE WILL DELETE ALL DATA IN OUR CURRENT DIRECTORY\n",
    "#DO NOT RUN IF YOU DON'T WANT THE DATA DELETED\n",
    "\n",
    "! rm -r clean_data\n",
    "! rm -r chest_xray\n",
    "! rm validation.lst\n",
    "! rm training.lst\n",
    "! rm testing.lst\n",
    "! rm im2rec.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
