{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'image-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'capstone_data'\n",
    "s3_dataPath = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "train_location = os.path.join(s3_dataPath, 'train')\n",
    "val_location = os.path.join(s3_dataPath, 'val')\n",
    "test_location = os.path.join(s3_dataPath, 'test')\n",
    "\n",
    "s3_lstPath = 's3://{}/lst_files'.format(bucket)\n",
    "\n",
    "train_lst = os.path.join(s3_lstPath, 'training.lst')\n",
    "val_lst = os.path.join(s3_lstPath, 'validation.lst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-991170486756/capstone_data/train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Image Classifier\n",
    "\n",
    "We will be using the HyperParameter Tuning system of AWS, so this process is more complicated than usual. However, it will yield the best results by finding the best model out of a random set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the AWS Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet = sagemaker.estimator.Estimator(role=role, \n",
    "                                       sagemaker_session=sagemaker_session, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.p2.xlarge',\n",
    "                                       output_path=\"s3://{}/{}/output\".format(bucket, 'ResNet'), \n",
    "                                       image_name=container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamaters\n",
    "\n",
    "You can read the documentation for all the Hyperparameters of the Image Classification Algorithm <a href=https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html>here</a>.<br>\n",
    "<ul>\n",
    "    <li><b>(Required) num_classes</b>: This is the number of output classes for the new dataset. Ours will be 2 for normal or pneumonia</li>\n",
    "    <li><b>(Required) num_training_samples</b>: This is the total number of training samples. The training set has 5,216 images in total.</li>\n",
    "    <li><b>num_layers</b>: The number of layers (depth) for the network. We use will 152 layers</li>\n",
    "    <li><b>image_shape</b>: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.</li>\n",
    "    <li><b>mini_batch_size</b>: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run.</li>\n",
    "    <li><b>epochs</b>: Number of training epochs.</li>\n",
    "    <li><b>learning_rate</b>: Learning rate for training.</li>\n",
    "    <li><b>precision_dtype</b>: Training datatype precision (default: float32). If set to 'float16', the training will be done in mixed_precision mode and will be faster than float32 mode</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet.set_hyperparameters(num_layers=152, #152 layers for ResNet-152\n",
    "                           image_shape= \"3,224,224\", #The dimensions of our cleaned images\n",
    "                           mini_batch_size=32, #Each batch will have 64 images\n",
    "                           epochs=10, #10 epochs or training iterations through the training set\n",
    "                           learning_rate = 0.01, #Learning rate used for the optimizer of the model which is sgd\n",
    "                           precision_dtype = 'float32',\n",
    "                           num_training_samples=5215,\n",
    "                           num_classes=2\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "I will tune the model by some of the embedded Image Classifier hyperparameters, you can read more <a href=https://docs.aws.amazon.com/sagemaker/latest/dg/IC-tuning.html>here</a>. The goal would be to select the best model that is chosen by the highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner, CategoricalParameter\n",
    "\n",
    "ResNet_hyperparameter_tuner = HyperparameterTuner(estimator=ResNet, \n",
    "                                                  objective_metric_name='validation:accuracy', \n",
    "                                                  objective_type='Maximize', \n",
    "                                                  max_jobs=5, #Going to run 20 different models\n",
    "                                                  max_parallel_jobs=1, #Going to train 3 models at the same time\n",
    "                                                  hyperparameter_ranges={\n",
    "                                                      #'mini_batch_size':IntegerParameter(64, 128),\n",
    "                                                      'learning_rate':ContinuousParameter(0.01, 0.1, scaling_type='Logarithmic'),\n",
    "                                                      'optimizer': CategoricalParameter(['sgd', 'adam', \n",
    "                                                                                              'rmsprop', 'nag'])\n",
    "                                                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(train_location, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(val_location, distribution='FullyReplicated', \n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "train_data_lst = sagemaker.session.s3_input(train_lst, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data_lst = sagemaker.session.s3_input(val_lst, distribution='FullyReplicated', \n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train':train_data, 'validation': validation_data, \n",
    "                 'train_lst':train_data_lst, 'validation_lst':validation_data_lst}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Hyperparameter Tuning job\n",
    "\n",
    "Here is where AWS will train multiple training jobs to find the best model out of a number of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_hyperparameter_tuner.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................._s\n"
     ]
    }
   ],
   "source": [
    "ResNet_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training numerous models, I will find the one that provided the best training job. <br><br>\n",
    "Which is here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image-classification-200112-2133-002-5c8e0b5a'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet_hyperparameter_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to create an AWS <a href=https://sagemaker.readthedocs.io/en/stable/estimators.html>Estimator</a> that uses the training job's model artifacts from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-12 23:14:27 Starting - Preparing the instances for training\n",
      "2020-01-12 23:14:27 Downloading - Downloading input data\n",
      "2020-01-12 23:14:27 Training - Training image download completed. Training in progress.\n",
      "2020-01-12 23:14:27 Uploading - Uploading generated training model\n",
      "2020-01-12 23:14:27 Completed - Training job completed\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.05561869868809854', u'optimizer': u'adam', u'_tuning_objective_metric': u'validation:accuracy', u'precision_dtype': u'float32', u'epochs': u'10', u'num_training_samples': u'5215', u'num_layers': u'152', u'mini_batch_size': u'32', u'image_shape': u'3,224,224', u'num_classes': u'2'}\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Final configuration: {u'optimizer': u'adam', u'_tuning_objective_metric': u'validation:accuracy', u'learning_rate': u'0.05561869868809854', u'epochs': u'10', u'lr_scheduler_factor': 0.1, u'num_layers': u'152', u'precision_dtype': u'float32', u'mini_batch_size': u'32', u'num_classes': u'2', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': 0, u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'5215'}\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:25 INFO 139726532032320] Creating record files for training.lst\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Done creating record files...\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Creating record files for validation.lst\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Done creating record files...\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] multi_label: 0\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Performing random weight initialization\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] num_layers: 152\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] epochs: 10\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] optimizer: adam\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] beta_1: 0.9\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] beta_2: 0.999\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] eps: 1e-08\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] learning_rate: 0.0556186986881\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] num_training_samples: 5215\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] mini_batch_size: 32\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] num_classes: 2\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] kv_store: device\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] tuning_objective_metric: validation:accuracy\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] --------------------\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:28:31 INFO 139726532032320] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[22:28:38] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1888.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:29:19 INFO 139726532032320] Epoch[0] Batch [20]#011Speed: 15.148 samples/sec#011accuracy=0.700893\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:29:51 INFO 139726532032320] Epoch[0] Batch [40]#011Speed: 17.215 samples/sec#011accuracy=0.727896\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:30:24 INFO 139726532032320] Epoch[0] Batch [60]#011Speed: 18.003 samples/sec#011accuracy=0.736168\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:30:56 INFO 139726532032320] Epoch[0] Batch [80]#011Speed: 18.406 samples/sec#011accuracy=0.737654\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:31:29 INFO 139726532032320] Epoch[0] Batch [100]#011Speed: 18.643 samples/sec#011accuracy=0.738861\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:32:01 INFO 139726532032320] Epoch[0] Batch [120]#011Speed: 18.799 samples/sec#011accuracy=0.735795\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:32:34 INFO 139726532032320] Epoch[0] Batch [140]#011Speed: 18.903 samples/sec#011accuracy=0.738918\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:07 INFO 139726532032320] Epoch[0] Batch [160]#011Speed: 18.980 samples/sec#011accuracy=0.740295\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:08 INFO 139726532032320] Epoch[0] Train-accuracy=0.740934\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:08 INFO 139726532032320] Epoch[0] Time cost=271.388\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:08 INFO 139726532032320] Epoch[0] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:33:43 INFO 139726532032320] Epoch[1] Batch [20]#011Speed: 18.751 samples/sec#011accuracy=0.763393\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:34:16 INFO 139726532032320] Epoch[1] Batch [40]#011Speed: 19.130 samples/sec#011accuracy=0.798780\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:34:48 INFO 139726532032320] Epoch[1] Batch [60]#011Speed: 19.271 samples/sec#011accuracy=0.815061\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:35:21 INFO 139726532032320] Epoch[1] Batch [80]#011Speed: 19.334 samples/sec#011accuracy=0.823302\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:35:54 INFO 139726532032320] Epoch[1] Batch [100]#011Speed: 19.374 samples/sec#011accuracy=0.827661\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:36:27 INFO 139726532032320] Epoch[1] Batch [120]#011Speed: 19.414 samples/sec#011accuracy=0.836519\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:36:59 INFO 139726532032320] Epoch[1] Batch [140]#011Speed: 19.437 samples/sec#011accuracy=0.842199\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:32 INFO 139726532032320] Epoch[1] Batch [160]#011Speed: 19.450 samples/sec#011accuracy=0.850155\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:34 INFO 139726532032320] Epoch[1] Train-accuracy=0.850887\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:34 INFO 139726532032320] Epoch[1] Time cost=264.871\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:35 INFO 139726532032320] Epoch[1] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:36 INFO 139726532032320] Storing the best model with validation accuracy: 0.500000\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:37:36 INFO 139726532032320] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:38:10 INFO 139726532032320] Epoch[2] Batch [20]#011Speed: 18.803 samples/sec#011accuracy=0.900298\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:38:43 INFO 139726532032320] Epoch[2] Batch [40]#011Speed: 19.155 samples/sec#011accuracy=0.912348\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:39:16 INFO 139726532032320] Epoch[2] Batch [60]#011Speed: 19.284 samples/sec#011accuracy=0.915984\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:39:48 INFO 139726532032320] Epoch[2] Batch [80]#011Speed: 19.352 samples/sec#011accuracy=0.914352\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:40:21 INFO 139726532032320] Epoch[2] Batch [100]#011Speed: 19.395 samples/sec#011accuracy=0.918007\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:40:54 INFO 139726532032320] Epoch[2] Batch [120]#011Speed: 19.423 samples/sec#011accuracy=0.919680\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:41:26 INFO 139726532032320] Epoch[2] Batch [140]#011Speed: 19.442 samples/sec#011accuracy=0.920213\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:41:59 INFO 139726532032320] Epoch[2] Batch [160]#011Speed: 19.455 samples/sec#011accuracy=0.922943\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:01 INFO 139726532032320] Epoch[2] Train-accuracy=0.923225\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:01 INFO 139726532032320] Epoch[2] Time cost=264.813\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:01 INFO 139726532032320] Epoch[2] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:42:36 INFO 139726532032320] Epoch[3] Batch [20]#011Speed: 18.662 samples/sec#011accuracy=0.936012\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:43:08 INFO 139726532032320] Epoch[3] Batch [40]#011Speed: 19.102 samples/sec#011accuracy=0.952744\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:43:41 INFO 139726532032320] Epoch[3] Batch [60]#011Speed: 19.237 samples/sec#011accuracy=0.946721\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:44:14 INFO 139726532032320] Epoch[3] Batch [80]#011Speed: 19.315 samples/sec#011accuracy=0.944444\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:44:47 INFO 139726532032320] Epoch[3] Batch [100]#011Speed: 19.362 samples/sec#011accuracy=0.946163\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:45:19 INFO 139726532032320] Epoch[3] Batch [120]#011Speed: 19.396 samples/sec#011accuracy=0.945506\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:45:52 INFO 139726532032320] Epoch[3] Batch [140]#011Speed: 19.420 samples/sec#011accuracy=0.945922\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:25 INFO 139726532032320] Epoch[3] Batch [160]#011Speed: 19.440 samples/sec#011accuracy=0.946040\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:26 INFO 139726532032320] Epoch[3] Train-accuracy=0.945988\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:26 INFO 139726532032320] Epoch[3] Time cost=265.016\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:28 INFO 139726532032320] Epoch[3] Validation-accuracy=0.687500\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:28 INFO 139726532032320] Storing the best model with validation accuracy: 0.687500\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:46:29 INFO 139726532032320] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:47:03 INFO 139726532032320] Epoch[4] Batch [20]#011Speed: 18.771 samples/sec#011accuracy=0.958333\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:47:36 INFO 139726532032320] Epoch[4] Batch [40]#011Speed: 19.140 samples/sec#011accuracy=0.946646\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:48:08 INFO 139726532032320] Epoch[4] Batch [60]#011Speed: 19.274 samples/sec#011accuracy=0.945184\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:48:41 INFO 139726532032320] Epoch[4] Batch [80]#011Speed: 19.343 samples/sec#011accuracy=0.943673\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:49:14 INFO 139726532032320] Epoch[4] Batch [100]#011Speed: 19.379 samples/sec#011accuracy=0.943069\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:49:47 INFO 139726532032320] Epoch[4] Batch [120]#011Speed: 19.407 samples/sec#011accuracy=0.943440\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:19 INFO 139726532032320] Epoch[4] Batch [140]#011Speed: 19.431 samples/sec#011accuracy=0.943484\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:52 INFO 139726532032320] Epoch[4] Batch [160]#011Speed: 19.450 samples/sec#011accuracy=0.942741\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:54 INFO 139726532032320] Epoch[4] Train-accuracy=0.942515\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:54 INFO 139726532032320] Epoch[4] Time cost=264.883\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:50:54 INFO 139726532032320] Epoch[4] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:51:29 INFO 139726532032320] Epoch[5] Batch [20]#011Speed: 18.713 samples/sec#011accuracy=0.949405\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:52:01 INFO 139726532032320] Epoch[5] Batch [40]#011Speed: 19.127 samples/sec#011accuracy=0.961128\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:52:34 INFO 139726532032320] Epoch[5] Batch [60]#011Speed: 19.267 samples/sec#011accuracy=0.957992\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:53:07 INFO 139726532032320] Epoch[5] Batch [80]#011Speed: 19.343 samples/sec#011accuracy=0.957948\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:53:39 INFO 139726532032320] Epoch[5] Batch [100]#011Speed: 19.387 samples/sec#011accuracy=0.948948\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:54:12 INFO 139726532032320] Epoch[5] Batch [120]#011Speed: 19.413 samples/sec#011accuracy=0.945764\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:54:45 INFO 139726532032320] Epoch[5] Batch [140]#011Speed: 19.432 samples/sec#011accuracy=0.946365\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:18 INFO 139726532032320] Epoch[5] Batch [160]#011Speed: 19.450 samples/sec#011accuracy=0.947399\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:19 INFO 139726532032320] Epoch[5] Train-accuracy=0.947724\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:19 INFO 139726532032320] Epoch[5] Time cost=264.877\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:21 INFO 139726532032320] Epoch[5] Validation-accuracy=0.625000\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:55:55 INFO 139726532032320] Epoch[6] Batch [20]#011Speed: 18.794 samples/sec#011accuracy=0.943452\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:56:28 INFO 139726532032320] Epoch[6] Batch [40]#011Speed: 19.166 samples/sec#011accuracy=0.958079\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:57:01 INFO 139726532032320] Epoch[6] Batch [60]#011Speed: 19.281 samples/sec#011accuracy=0.952869\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:57:34 INFO 139726532032320] Epoch[6] Batch [80]#011Speed: 19.349 samples/sec#011accuracy=0.954475\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:58:06 INFO 139726532032320] Epoch[6] Batch [100]#011Speed: 19.384 samples/sec#011accuracy=0.955446\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:58:39 INFO 139726532032320] Epoch[6] Batch [120]#011Speed: 19.418 samples/sec#011accuracy=0.957128\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:12 INFO 139726532032320] Epoch[6] Batch [140]#011Speed: 19.437 samples/sec#011accuracy=0.958555\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:45 INFO 139726532032320] Epoch[6] Batch [160]#011Speed: 19.455 samples/sec#011accuracy=0.957880\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:46 INFO 139726532032320] Epoch[6] Train-accuracy=0.957948\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:46 INFO 139726532032320] Epoch[6] Time cost=264.808\u001b[0m\n",
      "\u001b[34m[01/12/2020 22:59:46 INFO 139726532032320] Epoch[6] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:00:21 INFO 139726532032320] Epoch[7] Batch [20]#011Speed: 18.743 samples/sec#011accuracy=0.955357\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:00:54 INFO 139726532032320] Epoch[7] Batch [40]#011Speed: 19.145 samples/sec#011accuracy=0.963415\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:01:26 INFO 139726532032320] Epoch[7] Batch [60]#011Speed: 19.287 samples/sec#011accuracy=0.962602\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:01:59 INFO 139726532032320] Epoch[7] Batch [80]#011Speed: 19.356 samples/sec#011accuracy=0.961420\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:02:32 INFO 139726532032320] Epoch[7] Batch [100]#011Speed: 19.404 samples/sec#011accuracy=0.957611\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:03:04 INFO 139726532032320] Epoch[7] Batch [120]#011Speed: 19.436 samples/sec#011accuracy=0.958161\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:03:37 INFO 139726532032320] Epoch[7] Batch [140]#011Speed: 19.459 samples/sec#011accuracy=0.958777\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:10 INFO 139726532032320] Epoch[7] Batch [160]#011Speed: 19.472 samples/sec#011accuracy=0.958657\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:11 INFO 139726532032320] Epoch[7] Train-accuracy=0.958333\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:11 INFO 139726532032320] Epoch[7] Time cost=264.585\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:13 INFO 139726532032320] Epoch[7] Validation-accuracy=0.625000\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:04:47 INFO 139726532032320] Epoch[8] Batch [20]#011Speed: 18.793 samples/sec#011accuracy=0.950893\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:05:20 INFO 139726532032320] Epoch[8] Batch [40]#011Speed: 19.178 samples/sec#011accuracy=0.959604\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:05:53 INFO 139726532032320] Epoch[8] Batch [60]#011Speed: 19.305 samples/sec#011accuracy=0.961578\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:06:26 INFO 139726532032320] Epoch[8] Batch [80]#011Speed: 19.362 samples/sec#011accuracy=0.962577\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:06:58 INFO 139726532032320] Epoch[8] Batch [100]#011Speed: 19.405 samples/sec#011accuracy=0.958230\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:07:31 INFO 139726532032320] Epoch[8] Batch [120]#011Speed: 19.439 samples/sec#011accuracy=0.955579\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:04 INFO 139726532032320] Epoch[8] Batch [140]#011Speed: 19.466 samples/sec#011accuracy=0.957668\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:36 INFO 139726532032320] Epoch[8] Batch [160]#011Speed: 19.482 samples/sec#011accuracy=0.956910\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:38 INFO 139726532032320] Epoch[8] Train-accuracy=0.956983\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:38 INFO 139726532032320] Epoch[8] Time cost=264.442\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:08:38 INFO 139726532032320] Epoch[8] Validation-accuracy=nan\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:09:12 INFO 139726532032320] Epoch[9] Batch [20]#011Speed: 18.743 samples/sec#011accuracy=0.964286\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:09:45 INFO 139726532032320] Epoch[9] Batch [40]#011Speed: 19.158 samples/sec#011accuracy=0.964177\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:10:18 INFO 139726532032320] Epoch[9] Batch [60]#011Speed: 19.306 samples/sec#011accuracy=0.963627\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:10:50 INFO 139726532032320] Epoch[9] Batch [80]#011Speed: 19.377 samples/sec#011accuracy=0.967978\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:11:23 INFO 139726532032320] Epoch[9] Batch [100]#011Speed: 19.421 samples/sec#011accuracy=0.967203\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:11:56 INFO 139726532032320] Epoch[9] Batch [120]#011Speed: 19.451 samples/sec#011accuracy=0.966942\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:12:28 INFO 139726532032320] Epoch[9] Batch [140]#011Speed: 19.472 samples/sec#011accuracy=0.966312\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:01 INFO 139726532032320] Epoch[9] Batch [160]#011Speed: 19.491 samples/sec#011accuracy=0.965839\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:03 INFO 139726532032320] Epoch[9] Train-accuracy=0.966049\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:03 INFO 139726532032320] Epoch[9] Time cost=264.317\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:04 INFO 139726532032320] Epoch[9] Validation-accuracy=0.937500\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:05 INFO 139726532032320] Storing the best model with validation accuracy: 0.937500\u001b[0m\n",
      "\u001b[34m[01/12/2020 23:13:05 INFO 139726532032320] Saved checkpoint to \"/opt/ml/model/image-classification-0010.params\"\u001b[0m\n",
      "Training seconds: 2860\n",
      "Billable seconds: 2860\n"
     ]
    }
   ],
   "source": [
    "aws_estimator = sagemaker.estimator.Estimator.attach(training_job_name='image-classification-200112-2133-002-5c8e0b5a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constucting the ResNeXt 101-32x8d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
